# why LSTM solves vanishing gradient

print("WHY LSTM WORKS")
print()
print("key insight: cell state gradient")
print()
print("RNN: gradient *= W_hh at each step")
print("     -> exponential decay/growth")
print()
print("LSTM cell state update:")
print("  c_t = f_t * c_{t-1} + ...")
print()
print("gradient of c_t w.r.t. c_{t-1} = f_t")
print()
print("if f_t ~ 1:")
print("  gradient flows through unchanged!")
print("  no vanishing!")
print()
print("the forget gate can be close to 1")
print("allowing perfect gradient flow")
print()
print("LSTM learns WHEN to remember/forget")
print("rather than always decaying")
