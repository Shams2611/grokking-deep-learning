# classification summary

print("=" * 50)
print("SOFTMAX & CROSS-ENTROPY SUMMARY")
print("=" * 50)
print()
print("SOFTMAX")
print("  - converts logits to probabilities")
print("  - all outputs in [0, 1]")
print("  - all outputs sum to 1")
print("  - formula: exp(x_i) / sum(exp(x))")
print("  - use stable version: subtract max first")
print()
print("CROSS-ENTROPY LOSS")
print("  - measures probability error")
print("  - formula: -sum(target * log(pred))")
print("  - simplified: -log(prob_of_correct_class)")
print("  - heavily penalizes confident wrong answers")
print()
print("THE MAGIC GRADIENT")
print("  - d_loss/d_logits = softmax(logits) - target")
print("  - beautifully simple!")
print()
print("PIPELINE:")
print("  logits -> softmax -> probabilities")
print("  probabilities + target -> cross-entropy -> loss")
print("  gradient = probabilities - one_hot_target")
print()
print("use softmax for output layer of classifiers!")
