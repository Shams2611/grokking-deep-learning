# RNN summary

print("=" * 50)
print("RECURRENT NEURAL NETWORKS SUMMARY")
print("=" * 50)
print()
print("KEY CONCEPT")
print("  - process sequences one step at a time")
print("  - maintain hidden state (memory)")
print("  - same weights at every timestep")
print()
print("RNN CELL")
print("  h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b)")
print()
print("TRAINING: BPTT")
print("  - unroll network through time")
print("  - backpropagate through all steps")
print("  - sum gradients for shared weights")
print()
print("PROBLEMS")
print("  - vanishing gradients (long sequences)")
print("  - exploding gradients (use clipping)")
print()
print("SOLUTIONS")
print("  - gradient clipping")
print("  - truncated BPTT")
print("  - better architectures: LSTM, GRU")
print()
print("ARCHITECTURES")
print("  - many-to-one: classification")
print("  - many-to-many: sequence labeling")
print("  - bidirectional: use both directions")
print("  - deep/stacked: multiple layers")
print()
print("next: LSTM solves vanishing gradient!")
