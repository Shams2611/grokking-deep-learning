# word embeddings summary

print("=" * 50)
print("WORD EMBEDDINGS SUMMARY")
print("=" * 50)
print()
print("WHAT ARE EMBEDDINGS?")
print("  - dense vector representations of words")
print("  - capture semantic meaning")
print("  - similar words = similar vectors")
print()
print("WHY NOT ONE-HOT?")
print("  - too sparse and large")
print("  - no similarity information")
print()
print("WORD2VEC")
print("  - learn from context")
print("  - CBOW: context -> center word")
print("  - Skip-gram: center -> context words")
print("  - negative sampling for efficiency")
print()
print("KEY PROPERTIES")
print("  - king - man + woman â‰ˆ queen")
print("  - captures analogies and relationships")
print()
print("MODERN APPROACHES")
print("  - pretrained: GloVe, FastText")
print("  - subword: handle OOV words")
print("  - BPE: used in transformers")
print("  - contextual: BERT, GPT (different per context)")
print()
print("USAGE")
print("  - first layer in NLP models")
print("  - input to RNN/LSTM/Transformer")
print("  - fine-tune or freeze")
print()
print("embeddings = foundation of modern NLP!")
