# using pretrained embeddings

print("PRETRAINED EMBEDDINGS")
print()
print("training embeddings needs LOTS of data")
print("  - billions of words")
print("  - days of training")
print()
print("solution: use pretrained embeddings!")
print()
print("popular pretrained embeddings:")
print("  - Word2Vec (Google, 2013)")
print("  - GloVe (Stanford, 2014)")
print("  - FastText (Facebook, 2016)")
print()
print("how to use:")
print("  1. download pretrained vectors")
print("  2. load into embedding matrix")
print("  3. optionally fine-tune on your task")
print()
print("typical dimensions: 100, 200, 300")
print("vocab sizes: 100k - 1M+ words")
print()
print("GloVe example:")
print("  - 6B tokens, 400k vocab")
print("  - trained on Wikipedia + news")
print("  - captures semantic relationships")
